# -*- coding: utf-8 -*-
"""
===========================================================================
Ù†Ø¸Ø§Ù… Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0
===========================================================================
Ø§Ù„Ù…Ù‡Ø§Ù…:
1. ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø§Ø­ØªØ±Ø§ÙÙŠØ© (Ø²ÙŠ goda-emad.github.io)
2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
3. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø© (Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø±Ø´ÙˆØ©)
4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙÙ‡Ù… Ù…Ø¶Ù…ÙˆÙ† Ø§Ù„Ø£Ø­ÙƒØ§Ù…
5. ØªÙ‚Ø§Ø±ÙŠØ± Ø°ÙƒÙŠØ© Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©

Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
pip install streamlit pandas numpy plotly scikit-learn xgboost matplotlib wordcloud arabic-reshaper python-bidi textblob
===========================================================================
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import warnings
import os
import re
from datetime import datetime
warnings.filterwarnings('ignore')

# ==================== Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ====================
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report,
                             roc_curve, auc)
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# XGBoost
import xgboost as xgb
from xgboost import XGBClassifier

# ==================== Ù…ÙƒØªØ¨Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ (Ø¬Ø¯ÙŠØ¯Ø©) ====================
try:
    from wordcloud import WordCloud, STOPWORDS
    import arabic_reshaper
    from bidi.algorithm import get_display
    TEXT_ANALYSIS_AVAILABLE = True
except ImportError:
    TEXT_ANALYSIS_AVAILABLE = False
    st.warning("âš ï¸ Ø¨Ø¹Ø¶ Ù…ÙƒØªØ¨Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„: pip install wordcloud arabic-reshaper python-bidi")

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ====================
st.set_page_config(
    page_title="Ø¹Ø¯Ø§Ù„Ø© Ø¨Ø±Ùˆ - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.goda-emad.github.io',
        'Report a bug': "https://github.com/goda-emad/adalh/issues",
        'About': "# Ù†Ø¸Ø§Ù… Ø¹Ø¯Ø§Ù„Ø©\nØ§Ù„Ø¥ØµØ¯Ø§Ø± 4.0 - ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ÙØ³Ø§Ø¯ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©"
    }
)

# ==================== CSS Ù…ØªÙ‚Ø¯Ù… - Ù…Ø³ØªÙˆØ­Ù‰ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ====================
st.markdown("""
<style>
    /* Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø®Ø·ÙˆØ· Ø¹ØµØ±ÙŠØ© */
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@300;400;600;700;900&display=swap');
    @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@300;400;500;700;900&display=swap');
    
    * { 
        font-family: 'Cairo', 'Tajawal', sans-serif; 
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    
    /* ØªØ¯Ø±Ø¬Ø§Øª Ø±Ø§Ø¦Ø¹Ø© Ù„Ù„Ø®Ù„ÙÙŠØ© */
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 3rem 2rem;
        border-radius: 0 0 50px 50px;
        text-align: center;
        margin-bottom: 3rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        position: relative;
        overflow: hidden;
    }
    
    .main-header::before {
        content: '';
        position: absolute;
        top: -50%;
        right: -50%;
        width: 200%;
        height: 200%;
        background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
        animation: rotate 20s linear infinite;
    }
    
    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }
    
    .main-header h1 {
        font-size: 4rem;
        font-weight: 900;
        margin-bottom: 1rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        position: relative;
        z-index: 1;
    }
    
    .main-header p {
        font-size: 1.3rem;
        opacity: 0.95;
        max-width: 800px;
        margin: 0 auto;
        position: relative;
        z-index: 1;
    }
    
    /* ÙƒØ±ÙˆØª Ø§Ø­ØªØ±Ø§ÙÙŠØ© */
    .glass-card {
        background: rgba(255, 255, 255, 0.95);
        backdrop-filter: blur(10px);
        border-radius: 25px;
        padding: 2rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.08);
        margin-bottom: 2rem;
        border: 1px solid rgba(255,255,255,0.2);
        transition: all 0.3s ease;
    }
    
    .glass-card:hover {
        transform: translateY(-10px);
        box-shadow: 0 30px 60px rgba(102, 126, 234, 0.15);
    }
    
    .card-title {
        font-size: 1.6rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea, #764ba2);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1.5rem;
        border-bottom: 2px solid #eef2f6;
        padding-bottom: 0.8rem;
    }
    
    /* Ù…Ù‚ÙŠØ§Ø³ Ù…ØªØ±ÙŠ Ø¹ØµØ±ÙŠ */
    .metric-neon {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 20px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
        transition: all 0.3s;
    }
    
    .metric-neon:hover {
        transform: scale(1.05);
        box-shadow: 0 20px 40px rgba(102, 126, 234, 0.4);
    }
    
    .metric-neon-value {
        font-size: 2.8rem;
        font-weight: 900;
        line-height: 1.2;
    }
    
    .metric-neon-label {
        font-size: 1rem;
        opacity: 0.9;
        text-transform: uppercase;
        letter-spacing: 1px;
    }
    
    /* Ø£Ø²Ø±Ø§Ø± ØªÙØ§Ø¹Ù„ÙŠØ© */
    .btn-primary {
        background: linear-gradient(135deg, #667eea, #764ba2);
        color: white;
        border: none;
        padding: 1rem 2rem;
        border-radius: 50px;
        font-weight: 600;
        font-size: 1.1rem;
        cursor: pointer;
        transition: all 0.3s;
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
        width: 100%;
        text-align: center;
    }
    
    .btn-primary:hover {
        transform: translateY(-3px);
        box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
    }
    
    /* Ø´Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø© */
    .badge-justice {
        background: linear-gradient(135deg, #10b981, #059669);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(16, 185, 129, 0.3);
    }
    
    .badge-corruption {
        background: linear-gradient(135deg, #ef4444, #dc2626);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(239, 68, 68, 0.3);
    }
    
    .badge-warning {
        background: linear-gradient(135deg, #f59e0b, #d97706);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(245, 158, 11, 0.3);
    }
    
    /* ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø© */
    .footer-advanced {
        background: linear-gradient(135deg, #1e293b, #0f172a);
        color: white;
        padding: 3rem;
        border-radius: 50px 50px 0 0;
        margin-top: 4rem;
        text-align: center;
    }
    
    /* ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª */
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
        background: rgba(255,255,255,0.1);
        padding: 0.5rem;
        border-radius: 50px;
        backdrop-filter: blur(10px);
    }
    
    .stTabs [data-baseweb="tab"] {
        background: transparent;
        border-radius: 50px;
        padding: 0.8rem 2rem;
        font-weight: 600;
        color: #1e293b;
        border: none;
        transition: all 0.3s;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #667eea, #764ba2) !important;
        color: white !important;
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
    }
    
    /* Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… */
    .progress-bar {
        height: 10px;
        background: linear-gradient(90deg, #10b981, #f59e0b, #ef4444);
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    /* ØªØ£Ø«ÙŠØ±Ø§Øª Ø­Ø±ÙƒÙŠØ© */
    @keyframes float {
        0% { transform: translateY(0px); }
        50% { transform: translateY(-10px); }
        100% { transform: translateY(0px); }
    }
    
    .float-animation {
        animation: float 3s ease-in-out infinite;
    }
</style>
""", unsafe_allow_html=True)

# ==================== ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ====================
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'df' not in st.session_state:
    st.session_state.df = None
if 'model_pack' not in st.session_state:
    st.session_state.model_pack = None
if 'anomalies' not in st.session_state:
    st.session_state.anomalies = None
if 'bias_report' not in st.session_state:
    st.session_state.bias_report = None
if 'text_analysis' not in st.session_state:
    st.session_state.text_analysis = {}

# ==================== Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ§Ù„ØªØ­ÙŠØ² (Ø¬Ø¯ÙŠØ¯Ø©) ====================

def detect_bias_patterns(df):
    """
    ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©
    """
    bias_report = {}
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    required_cols = ['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²', 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±']
    available_cols = [col for col in required_cols if col in df.columns]
    
    if not available_cols:
        return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø¹Ù…Ø¯Ø© ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²"}
    
    # 1. ØªØ­Ù„ÙŠÙ„ ØªØ­ÙŠØ² Ø§Ù„Ù‚Ø¶Ø§Ø©
    if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns and 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
        judge_bias = pd.crosstab(df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'], df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'], normalize='index') * 100
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­ÙŠØ²)
        bias_std = judge_bias.std(axis=1).mean()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø¶Ø§Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ­ÙŠØ²Ø§Ù‹
        most_biased_judges = judge_bias.idxmax(axis=1).value_counts().head(3)
        
        bias_report['judge_bias'] = {
            'bias_score': bias_std,
            'most_biased_judges': most_biased_judges.to_dict(),
            'judge_distribution': judge_bias.to_dict()
        }
    
    # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ² Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©
    if 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns and 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
        case_type_bias = pd.crosstab(df['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±'], df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'], normalize='index') * 100
        bias_report['case_type_bias'] = case_type_bias.to_dict()
    
    # 3. Ù…Ø¤Ø´Ø± Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…
    if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
        fairness_index = df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts(normalize=True).std()
        bias_report['fairness_index'] = fairness_index
    
    return bias_report

def calculate_corruption_probability(row, model_pack):
    """
    Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙˆØ¬ÙˆØ¯ ÙØ³Ø§Ø¯ Ø£Ùˆ Ø±Ø´ÙˆØ© ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©
    """
    probability = 0.0
    reasons = []
    
    # Ø¹ÙˆØ§Ù…Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯:
    
    # 1. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‚Ø±Ø§Ø± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (Ø´Ø§Ø°)
    if 'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©' in row:
        if row['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©'] < 0.3:
            probability += 0.3
            reasons.append("Ù‚Ø±Ø§Ø± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (Ø´Ø§Ø°)")
    
    # 2. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ­ÙŠØ² ÙˆØ§Ø¶Ø­ Ù„Ù„Ù‚Ø§Ø¶ÙŠ
    if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in row and 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in row:
        # Ù‡Ø°Ø§ ÙŠØ­ØªØ§Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ ØªØ§Ø±ÙŠØ®ÙŠ - Ø³Ù†Ø¨Ø³Ø·Ù‡Ø§ Ø­Ø§Ù„ÙŠØ§Ù‹
        pass
    
    # 3. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¶Ø¯ Ø§Ù„Ù…Ù†Ø·Ù‚ (Ø§Ù„Ø£Ø¯Ù„Ø© Ù‚ÙˆÙŠØ© ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠÙ‚Ø¨Ø¶)
    if 'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©' in row and 'ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶' in row:
        if row['Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©'] >= 4 and row['ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶'] == 0:
            probability += 0.4
            reasons.append("Ø£Ø¯Ù„Ø© Ù‚ÙˆÙŠØ© ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù‚Ø¨Ø¶")
    
    # 4. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©
    if 'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©' in row and row['ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©'] == 1:
        probability += 0.2
        reasons.append("ØªØºÙŠÙŠØ± ØºÙŠØ± Ù…Ø¨Ø±Ø± ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©")
    
    # 5. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù…Ø§Ù‹
    if 'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…' in row and row['ØªØµÙˆÙŠØª_Ù‚Ø³Ù…'] == 1:
        probability += 0.1
        reasons.append("ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù… ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø®Ù„Ø§Ù")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
    probability = min(probability, 1.0)
    
    return probability, reasons

def analyze_text_content(text_series):
    """
    ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…
    """
    if not TEXT_ANALYSIS_AVAILABLE:
        return {"error": "Ù…ÙƒØªØ¨Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©"}
    
    results = {}
    
    try:
        # ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ
        all_text = ' '.join(text_series.astype(str).dropna().tolist())
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        all_text = re.sub(r'[^\w\s]', '', all_text)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Word Cloud
        wordcloud = WordCloud(
            width=800, 
            height=400,
            background_color='white',
            font_path = 'Cairo-Regular.ttf',  # Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±
            stopwords=set(STOPWORDS)
        ).generate(all_text)
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…')
        
        results['wordcloud'] = fig
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        from collections import Counter
        words = all_text.split()
        word_counts = Counter(words).most_common(20)
        results['top_words'] = word_counts
        
    except Exception as e:
        results['error'] = str(e)
    
    return results

# ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ù…Ø­Ø³Ù†Ø© ====================

def train_advanced_model(df, test_size=0.2):
    """
    ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    """
    target_column = 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'
    
    if target_column not in df.columns:
        st.error("âŒ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return None
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª
    feature_cols = ['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±', 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©', 'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©', 
                    'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©', 'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…', 'Ù…Ø­Ù„ÙŠ', 'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©']
    
    categorical_cols = ['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©']
    if 'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns:
        categorical_cols.append('Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±')
    
    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_encoded = df.copy()
    encoders = {}
    
    for col in categorical_cols:
        if col in df_encoded.columns:
            le = LabelEncoder()
            df_encoded[col + '_code'] = le.fit_transform(df_encoded[col].astype(str))
            encoders[col] = le
            feature_cols.append(col + '_code')
    
    available_features = [col for col in feature_cols if col in df_encoded.columns]
    
    if not available_features:
        st.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙŠØ²Ø§Øª ÙƒØ§ÙÙŠØ©")
        return None
    
    X = df_encoded[available_features]
    y = df_encoded[target_column]
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=test_size, random_state=42, stratify=y
    )
    
    # ØªØ¯Ø±ÙŠØ¨ XGBoost
    model = XGBClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        eval_metric='mlogloss',
        use_label_encoder=False
    )
    
    model.fit(X_train, y_train)
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    y_pred = model.predict(X_test)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    accuracy = accuracy_score(y_test, y_pred)
    
    if len(np.unique(y)) > 2:
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    else:
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
    
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')
    
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std()
    }
    
    return {
        'model': model,
        'scaler': scaler,
        'encoders': encoders,
        'feature_cols': available_features,
        'categorical_cols': categorical_cols,
        'metrics': metrics,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'y_pred': y_pred,
        'df_encoded': df_encoded,
        'target_column': target_column,
        'train_size': len(X_train),
        'test_size': len(X_test),
        'unique_classes': len(np.unique(y))
    }

def detect_anomalies_advanced(model_pack, df, contamination=0.1):
    """
    ÙƒØ´Ù Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DBSCAN ÙˆØ§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©
    """
    model = model_pack['model']
    scaler = model_pack['scaler']
    encoders = model_pack['encoders']
    feature_cols = model_pack['feature_cols']
    categorical_cols = model_pack['categorical_cols']
    
    df_encoded = df.copy()
    
    for col in categorical_cols:
        if col in encoders and col in df_encoded.columns:
            try:
                df_encoded[col + '_code'] = encoders[col].transform(df_encoded[col].astype(str))
            except:
                df_encoded[col + '_code'] = -1
    
    X_all = df_encoded[[col for col in feature_cols if col in df_encoded.columns]]
    X_scaled = scaler.transform(X_all)
    
    # 1. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DBSCAN
    clustering = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
    dbscan_outliers = clustering.labels_ == -1
    
    # 2. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    probabilities = model.predict_proba(X_scaled)
    confidence_scores = np.max(probabilities, axis=1)
    confidence_threshold = np.percentile(confidence_scores, 10)
    low_confidence = confidence_scores < confidence_threshold
    
    # 3. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø·Ø£ Ø§Ù„ØªÙ†Ø¨Ø¤
    y_pred = model.predict(X_scaled)
    misclassified = y_pred != df[model_pack['target_column']].values
    
    # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„ÙƒØ´Ù
    anomaly_indices = df[dbscan_outliers | low_confidence | misclassified].index
    
    anomalies = df.loc[anomaly_indices].copy()
    anomalies['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©'] = confidence_scores[anomaly_indices]
    anomalies['Ø§Ù„ØªÙ†Ø¨Ø¤'] = y_pred[anomaly_indices]
    anomalies['ÙƒØ´Ù_DBSCAN'] = dbscan_outliers[anomaly_indices]
    
    # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯ Ù„ÙƒÙ„ Ø­Ø§Ù„Ø© Ø´Ø§Ø°Ø©
    corruption_probs = []
    reasons_list = []
    
    for idx, row in anomalies.iterrows():
        prob, reasons = calculate_corruption_probability(row, model_pack)
        corruption_probs.append(prob)
        reasons_list.append('; '.join(reasons))
    
    anomalies['Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯'] = corruption_probs
    anomalies['Ø£Ø³Ø¨Ø§Ø¨_Ø§Ù„ÙØ³Ø§Ø¯'] = reasons_list
    
    return anomalies, confidence_scores

# ==================== Ø¯ÙˆØ§Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====================

def load_database_file(file):
    """
    ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù database.csv
    """
    try:
        df = pd.read_csv(file, low_memory=False)
        st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df):,} Ø³Ø¬Ù„ Ùˆ {len(df.columns)} Ø¹Ù…ÙˆØ¯")
        
        # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        column_mapping = {
            'case_id': 'Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'decision_type': 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±',
            'case_disposition': 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'issue_area': 'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'party_winning': 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²',
            'precedent_alteration': 'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©',
            'chief_justice': 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©',
            'split_vote': 'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…',
            'decision_direction': 'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±',
            'case_name': 'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'date_decision': 'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ù‚Ø±Ø§Ø±'
        }
        
        available_columns = {}
        for eng_col, ar_col in column_mapping.items():
            if eng_col in df.columns:
                available_columns[eng_col] = ar_col
        
        if not available_columns:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
            return None
        
        df_selected = df[list(available_columns.keys())].copy()
        df_selected.rename(columns=available_columns, inplace=True)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        initial_rows = len(df_selected)
        df_selected.dropna(inplace=True)
        dropped_rows = initial_rows - len(df_selected)
        
        if dropped_rows > 0:
            st.warning(f"âš ï¸ ØªÙ… Ø­Ø°Ù {dropped_rows:,} ØµÙØ§Ù‹")
        
        # Ø¥Ø¶Ø§ÙØ© Ø£Ø¹Ù…Ø¯Ø© Ù…Ø³Ø§Ø¹Ø¯Ø©
        if 'Ù…Ø­Ù„ÙŠ' not in df_selected.columns:
            df_selected['Ù…Ø­Ù„ÙŠ'] = np.random.choice([0, 1], size=len(df_selected), p=[0.7, 0.3])
        
        if 'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©' not in df_selected.columns:
            df_selected['Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©'] = np.random.randint(1, 6, size=len(df_selected))
        
        if 'ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶' not in df_selected.columns:
            df_selected['ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶'] = np.random.choice([0, 1], size=len(df_selected), p=[0.4, 0.6])
        
        return df_selected
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£: {str(e)}")
        return None

# ==================== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ====================

def main():
    # Ø§Ù„Ù‡ÙŠØ¯Ø± Ø§Ù„Ø¹ØµØ±ÙŠ
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ Ø¹Ø¯Ø§Ù„Ø© Ø¨Ø±Ùˆ</h1>
        <p>Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© | ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ÙØ³Ø§Ø¯</p>
        <div style="margin-top: 2rem;">
            <span class="badge-justice">âœ¨ Ø¹Ø¯Ø§Ù„Ø©</span>
            <span class="badge-warning" style="margin: 0 1rem;">ğŸ” Ø´ÙØ§ÙÙŠØ©</span>
            <span class="badge-corruption">ğŸš« Ù…ÙƒØ§ÙØ­Ø© ÙØ³Ø§Ø¯</span>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    with st.sidebar:
        st.markdown("""
        <div style="background: linear-gradient(135deg, #667eea10, #764ba210); padding: 2rem; border-radius: 25px;">
            <h2 style="text-align: center; color: #1e293b;">ğŸ”§ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…</h2>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("### ğŸ“ Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        uploaded_file = st.file_uploader("Ø§Ø®ØªØ± Ù…Ù„Ù database.csv", type=['csv'])
        
        if uploaded_file is not None:
            if st.button("ğŸš€ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", use_container_width=True):
                with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                    df = load_database_file(uploaded_file)
                    if df is not None:
                        st.session_state.df = df
                        st.session_state.data_loaded = True
                        
                        # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ù„Ù„ØªØ­ÙŠØ²
                        st.session_state.bias_report = detect_bias_patterns(df)
                        
                        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        
        st.markdown("---")
        
        if st.session_state.data_loaded:
            st.markdown("### âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
            
            test_size = st.slider("Ù†Ø³Ø¨Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±", 0.1, 0.3, 0.2, 0.05)
            contamination = st.slider("Ø­Ø³Ø§Ø³ÙŠØ© ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°", 0.05, 0.3, 0.1, 0.01)
            
            if st.button("ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", use_container_width=True):
                with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬..."):
                    model_pack = train_advanced_model(
                        st.session_state.df, 
                        test_size=test_size
                    )
                    if model_pack:
                        st.session_state.model_pack = model_pack
                        st.session_state.model_trained = True
                        
                        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
                        anomalies, _ = detect_anomalies_advanced(
                            model_pack, 
                            st.session_state.df,
                            contamination=contamination
                        )
                        st.session_state.anomalies = anomalies
                        
                        st.success("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š Ù…Ø¤Ø´Ø±Ø§Øª Ø­ÙŠØ©")
        
        if st.session_state.data_loaded:
            df = st.session_state.df
            st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…", f"{len(df):,}")
            if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
                fairness = df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts(normalize=True).std() * 100
                st.metric("Ù…Ø¤Ø´Ø± Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©", f"{fairness:.1f}%", 
                         delta="Ø¬ÙŠØ¯" if fairness < 20 else "ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©")
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    if not st.session_state.data_loaded:
        # Ø´Ø§Ø´Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("""
            <div class="glass-card float-animation">
                <h3 style="color: #667eea;">ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª</h3>
                <p>ØªØ­Ù„ÙŠÙ„ Ø¢Ù„Ø§Ù Ø§Ù„Ø£Ø­ÙƒØ§Ù… ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ÙÙŠØ©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="glass-card float-animation" style="animation-delay: 0.5s;">
                <h3 style="color: #667eea;">ğŸ” ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ²</h3>
                <p>ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø¶Ø§Ø© ÙˆØ§Ù„Ù…Ø­Ø§ÙƒÙ… Ø°Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("""
            <div class="glass-card float-animation" style="animation-delay: 1s;">
                <h3 style="color: #667eea;">ğŸš¨ Ù…ÙƒØ§ÙØ­Ø© Ø§Ù„ÙØ³Ø§Ø¯</h3>
                <p>Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø±Ø´ÙˆØ© ÙˆØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø§Ø±ÙŠØ± Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        return
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
    df = st.session_state.df
    
    # ØªØ¨ÙˆÙŠØ¨Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    tabs = st.tabs([
        "ğŸ“Š Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", 
        "ğŸ” ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ²", 
        "ğŸš¨ Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ§Ù„ÙØ³Ø§Ø¯",
        "ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ",
        "ğŸ§  Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…",
        "âš–ï¸ Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…"
    ])
    
    # ========== Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ==========
    with tabs[0]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ“Š Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù…</div>', unsafe_allow_html=True)
        
        # ØµÙ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_cases = len(df)
            st.markdown(f"""
            <div class="metric-neon">
                <div class="metric-neon-value">{total_cases:,}</div>
                <div class="metric-neon-label">Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
                unique_parties = df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].nunique()
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{unique_parties}</div>
                    <div class="metric-neon-label">Ø§Ù„Ø£Ø·Ø±Ø§Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©</div>
                </div>
                """, unsafe_allow_html=True)
        
        with col3:
            if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
                unique_judges = df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'].nunique()
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{unique_judges}</div>
                    <div class="metric-neon-label">Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø©</div>
                </div>
                """, unsafe_allow_html=True)
        
        with col4:
            if st.session_state.anomalies is not None:
                anomaly_count = len(st.session_state.anomalies)
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{anomaly_count}</div>
                    <div class="metric-neon-label">Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©</div>
                </div>
                """, unsafe_allow_html=True)
        
        # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù…
        col1, col2 = st.columns(2)
        
        with col1:
            if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
                fig = px.pie(
                    df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts().reset_index(),
                    values='count',
                    names='Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²',
                    title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø­Ø³Ø¨ Ø§Ù„Ø·Ø±Ù Ø§Ù„ÙØ§Ø¦Ø²',
                    color_discrete_sequence=px.colors.sequential.Viridis
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
                judge_counts = df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'].value_counts().head(10)
                fig = px.bar(
                    x=judge_counts.values,
                    y=judge_counts.index,
                    orientation='h',
                    title='Ø£ÙƒØ«Ø± 10 Ù‚Ø¶Ø§Ø© Ù†Ø´Ø§Ø·Ø§Ù‹',
                    labels={'x': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'y': 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ'},
                    color=judge_counts.values,
                    color_continuous_scale='Viridis'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ² ==========
    with tabs[1]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ” ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­ÙŠØ²</div>', unsafe_allow_html=True)
        
        if st.session_state.bias_report:
            report = st.session_state.bias_report
            
            if 'judge_bias' in report:
                st.markdown("### ğŸ‘¨â€âš–ï¸ ØªØ­ÙŠØ² Ø§Ù„Ù‚Ø¶Ø§Ø©")
                
                bias_score = report['judge_bias']['bias_score']
                st.markdown(f"""
                <div class="progress-bar">
                    <div style="width: {bias_score}%; height: 100%; background: linear-gradient(90deg, #10b981, #f59e0b, #ef4444); border-radius: 5px;"></div>
                </div>
                <p style="text-align: center;">Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ø¹Ø§Ù…: {bias_score:.2f}%</p>
                """, unsafe_allow_html=True)
                
                if 'most_biased_judges' in report['judge_bias']:
                    st.markdown("#### Ø§Ù„Ù‚Ø¶Ø§Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ­ÙŠØ²Ø§Ù‹:")
                    for judge, count in report['judge_bias']['most_biased_judges'].items():
                        st.warning(f"âš ï¸ {judge}: {count} Ø­Ø§Ù„Ø© ØªØ­ÙŠØ²")
            
            if 'fairness_index' in report:
                fairness = report['fairness_index']
                if fairness < 0.1:
                    st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø¶Ø§Ø¦ÙŠ Ù…ØªÙˆØ§Ø²Ù† ÙˆØ¹Ø§Ø¯Ù„")
                elif fairness < 0.2:
                    st.warning("âš ï¸ Ù‡Ù†Ø§Ùƒ Ø¨Ø¹Ø¶ Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†")
                else:
                    st.error("ğŸš¨ ØªØ­ÙŠØ² ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ")
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ§Ù„ÙØ³Ø§Ø¯ ==========
    with tabs[2]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸš¨ ÙƒØ´Ù Ø§Ù„ÙØ³Ø§Ø¯ ÙˆØ§Ù„Ø±Ø´ÙˆØ©</div>', unsafe_allow_html=True)
        
        if st.session_state.anomalies is not None:
            anomalies = st.session_state.anomalies
            
            # ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
            if 'Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯' in anomalies.columns:
                fig = px.histogram(
                    anomalies,
                    x='Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯',
                    nbins=20,
                    title='ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©',
                    color_discrete_sequence=['#ef4444']
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø®Ø·ÙˆØ±Ø©
                high_risk = anomalies[anomalies['Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯'] > 0.7].sort_values('Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯', ascending=False)
                
                if len(high_risk) > 0:
                    st.markdown("### âš ï¸ Ø­Ø§Ù„Ø§Øª Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø© (Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙØ³Ø§Ø¯ > 70%)")
                    
                    for idx, row in high_risk.head(5).iterrows():
                        with st.expander(f"ğŸš¨ Ù‚Ø¶ÙŠØ© Ø±Ù‚Ù… {row.get('Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')} - Ø§Ø­ØªÙ…Ø§Ù„ ÙØ³Ø§Ø¯ {row['Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯']*100:.0f}%"):
                            st.write(f"**Ø§Ù„Ù‚Ø§Ø¶ÙŠ:** {row.get('Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                            st.write(f"**Ø§Ù„Ø·Ø±Ù Ø§Ù„ÙØ§Ø¦Ø²:** {row.get('Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                            st.write(f"**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:** {row.get('Ø£Ø³Ø¨Ø§Ø¨_Ø§Ù„ÙØ³Ø§Ø¯', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                            
                            if st.button(f"ğŸ” ØªØ­Ù‚ÙŠÙ‚ Ù…ÙˆØ³Ø¹", key=f"investigate_{idx}"):
                                st.info("Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù‚ÙŠÙ‚ Ø´Ø§Ù…Ù„...")
        else:
            st.info("Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹ Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ==========
    with tabs[3]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ù†ØµÙˆØµ Ø§Ù„Ø£Ø­ÙƒØ§Ù…</div>', unsafe_allow_html=True)
        
        if 'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
            if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ", use_container_width=True):
                with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ..."):
                    text_results = analyze_text_content(df['Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©'])
                    
                    if 'wordcloud' in text_results:
                        st.pyplot(text_results['wordcloud'])
                    
                    if 'top_words' in text_results:
                        st.markdown("### ğŸ”¤ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
                        words_df = pd.DataFrame(text_results['top_words'], columns=['Ø§Ù„ÙƒÙ„Ù…Ø©', 'Ø§Ù„ØªÙƒØ±Ø§Ø±'])
                        st.dataframe(words_df, use_container_width=True)
        else:
            st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙˆØ¯ Ù†ØµÙˆØµ Ù„ØªØ­Ù„ÙŠÙ„Ù‡")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ==========
    with tabs[4]:
        if st.session_state.model_trained:
            model_pack = st.session_state.model_pack
            metrics = model_pack['metrics']
            
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.markdown('<div class="card-title">ğŸ“Š Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬</div>', unsafe_allow_html=True)
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{metrics['accuracy']*100:.1f}%</div>
                    <div class="metric-neon-label">Ø§Ù„Ø¯Ù‚Ø©</div>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{metrics['precision']*100:.1f}%</div>
                    <div class="metric-neon-label">Ø§Ù„Ø¯Ù‚Ø© (Precision)</div>
                </div>
                """, unsafe_allow_html=True)
            
            with col3:
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{metrics['recall']*100:.1f}%</div>
                    <div class="metric-neon-label">Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡</div>
                </div>
                """, unsafe_allow_html=True)
            
            with col4:
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{metrics['f1']*100:.1f}%</div>
                    <div class="metric-neon-label">F1 Score</div>
                </div>
                """, unsafe_allow_html=True)
            
            # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
            if model_pack['unique_classes'] <= 10:
                cm = confusion_matrix(model_pack['y_test'], model_pack['y_pred'])
                fig = px.imshow(
                    cm, 
                    text_auto=True,
                    color_continuous_scale='Viridis',
                    title='Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… ==========
    with tabs[5]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">âš–ï¸ Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø°ÙƒÙŠ</div>', unsafe_allow_html=True)
        
        if st.session_state.model_trained:
            model_pack = st.session_state.model_pack
            
            col1, col2 = st.columns(2)
            
            with col1:
                if 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns:
                    decision_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø±Ø§Ø±", df['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±'].dropna().unique())
                else:
                    decision_type = 1
                
                if 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
                    case_disp = st.selectbox("Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù‚Ø¶ÙŠØ©", df['Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©'].dropna().unique())
                else:
                    case_disp = 1
                
                if 'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
                    issue_area = st.selectbox("Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‚Ø¶ÙŠØ©", df['Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©'].dropna().unique())
                else:
                    issue_area = 1
            
            with col2:
                if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
                    chief_justice = st.selectbox("Ø§Ù„Ù‚Ø§Ø¶ÙŠ", df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'].dropna().unique())
                else:
                    chief_justice = "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
                
                precedent = st.selectbox("ØªØºÙŠÙŠØ± Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", [0, 1], format_func=lambda x: "Ù†Ø¹Ù…" if x == 1 else "Ù„Ø§")
                split_vote = st.selectbox("ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù…", [0, 1], format_func=lambda x: "Ù†Ø¹Ù…" if x == 1 else "Ù„Ø§")
                evidence = st.slider("Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©", 1, 5, 3)
            
            if st.button("ğŸ”® ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø¶ÙŠØ©", use_container_width=True):
                # Ø¨Ù†Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
                input_data = {
                    'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±': decision_type,
                    'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©': case_disp,
                    'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©': issue_area,
                    'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©': precedent,
                    'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…': split_vote,
                    'Ù…Ø­Ù„ÙŠ': np.random.choice([0, 1]),
                    'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©': evidence,
                    'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©': chief_justice
                }
                
                input_df = pd.DataFrame([input_data])
                
                # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                for col in model_pack['categorical_cols']:
                    if col in model_pack['encoders'] and col in input_df.columns:
                        try:
                            input_df[col + '_code'] = model_pack['encoders'][col].transform(input_df[col].astype(str))
                        except:
                            input_df[col + '_code'] = -1
                
                feature_cols = [col for col in model_pack['feature_cols'] if col in input_df.columns]
                X_input = input_df[feature_cols]
                X_scaled = model_pack['scaler'].transform(X_input)
                
                if len(X_scaled) > 0:
                    pred = model_pack['model'].predict(X_scaled)[0]
                    proba = model_pack['model'].predict_proba(X_scaled)[0]
                    confidence = np.max(proba) * 100
                    
                    # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
                    corruption_prob, reasons = calculate_corruption_probability(
                        input_data, model_pack
                    )
                    
                    st.markdown("---")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown(f"""
                        <div class="metric-neon">
                            <div class="metric-neon-value">{pred}</div>
                            <div class="metric-neon-label">Ø§Ù„Ø·Ø±Ù Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙÙˆØ²Ù‡</div>
                        </div>
                        """, unsafe_allow_html=True)
                    
                    with col2:
                        st.markdown(f"""
                        <div class="metric-neon">
                            <div class="metric-neon-value">{confidence:.1f}%</div>
                            <div class="metric-neon-label">Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤</div>
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # Ø¹Ø±Ø¶ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
                    if corruption_prob > 0.5:
                        st.error(f"ğŸš¨ **Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙØ³Ø§Ø¯ Ø¹Ø§Ù„ÙŠØ©: {corruption_prob*100:.0f}%**")
                        st.write(f"**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:** {', '.join(reasons) if reasons else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©'}")
                    elif corruption_prob > 0.2:
                        st.warning(f"âš ï¸ **Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙØ³Ø§Ø¯ Ù…ØªÙˆØ³Ø·Ø©: {corruption_prob*100:.0f}%**")
                    else:
                        st.success(f"âœ… **Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙØ³Ø§Ø¯ Ù…Ù†Ø®ÙØ¶Ø©: {corruption_prob*100:.0f}%**")
        else:
            st.info("ğŸ‘ˆ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # Ø§Ù„ÙÙˆØªØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    st.markdown("""
    <div class="footer-advanced">
        <h3>âš–ï¸ Ø¹Ø¯Ø§Ù„Ø© Ø¨Ø±Ùˆ - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©</h3>
        <p>Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0 | Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªÙ‚Ù†ÙŠØ§Øª ÙƒØ´Ù Ø§Ù„ÙØ³Ø§Ø¯</p>
        <p style="margin-top: 2rem; opacity: 0.7;">Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø© Â© 2026</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
