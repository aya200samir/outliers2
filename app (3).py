# -*- coding: utf-8 -*-
"""
===========================================================================
Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©
===========================================================================
Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 5.0 (Ù†Ø³Ø®Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Decision Tree, KNN, Ùˆ Vectors)

Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
    streamlit, pandas, numpy, plotly, scikit-learn, xgboost, matplotlib
    wordcloud, arabic-reshaper, python-bidi, textblob, shap, scipy
===========================================================================
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import warnings
import os
import re
from datetime import datetime
import time
from collections import Counter
warnings.filterwarnings('ignore')

# ==================== Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ====================
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report,
                             roc_curve, auc, roc_auc_score)
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from scipy.spatial.distance import cdist

# XGBoost
import xgboost as xgb
from xgboost import XGBClassifier

# SHAP Ù„Ù„ØªÙØ³ÙŠØ±
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

# ==================== Ù…ÙƒØªØ¨Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ====================
try:
    from wordcloud import WordCloud, STOPWORDS
    import arabic_reshaper
    from bidi.algorithm import get_display
    from textblob import TextBlob
    TEXT_ANALYSIS_AVAILABLE = True
except ImportError:
    TEXT_ANALYSIS_AVAILABLE = False

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ====================
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/adalh-project',
        'Report a bug': "https://github.com/adalh-project/issues",
        'About': "# Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©\nØ§Ù„Ø¥ØµØ¯Ø§Ø± 5.0 - Ù…Ø¹ Decision Tree Ùˆ KNN Ùˆ Vectors"
    }
)

# ==================== CSS Ù…ØªÙ‚Ø¯Ù… ====================
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@300;400;600;700;900&display=swap');
    @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@300;400;500;700;900&display=swap');
    
    * { 
        font-family: 'Cairo', 'Tajawal', sans-serif; 
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    
    .main-header {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        padding: 3rem 2rem;
        border-radius: 0 0 50px 50px;
        text-align: center;
        margin-bottom: 3rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        position: relative;
        overflow: hidden;
    }
    
    .main-header::before {
        content: '';
        position: absolute;
        top: -50%;
        right: -50%;
        width: 200%;
        height: 200%;
        background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
        animation: rotate 20s linear infinite;
    }
    
    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }
    
    .main-header h1 {
        font-size: 4rem;
        font-weight: 900;
        margin-bottom: 1rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        position: relative;
        z-index: 1;
    }
    
    .main-header p {
        font-size: 1.3rem;
        opacity: 0.95;
        max-width: 800px;
        margin: 0 auto;
        position: relative;
        z-index: 1;
    }
    
    .glass-card {
        background: rgba(255, 255, 255, 0.95);
        backdrop-filter: blur(10px);
        border-radius: 25px;
        padding: 2rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.08);
        margin-bottom: 2rem;
        border: 1px solid rgba(255,255,255,0.2);
        transition: all 0.3s ease;
    }
    
    .glass-card:hover {
        transform: translateY(-10px);
        box-shadow: 0 30px 60px rgba(30, 60, 114, 0.15);
    }
    
    .card-title {
        font-size: 1.6rem;
        font-weight: 700;
        background: linear-gradient(135deg, #1e3c72, #2a5298);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1.5rem;
        border-bottom: 2px solid #eef2f6;
        padding-bottom: 0.8rem;
    }
    
    .metric-neon {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        border-radius: 20px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 15px 30px rgba(30, 60, 114, 0.3);
        transition: all 0.3s;
    }
    
    .metric-neon:hover {
        transform: scale(1.05);
        box-shadow: 0 20px 40px rgba(30, 60, 114, 0.4);
    }
    
    .metric-neon-value {
        font-size: 2.8rem;
        font-weight: 900;
        line-height: 1.2;
    }
    
    .metric-neon-label {
        font-size: 1rem;
        opacity: 0.9;
        text-transform: uppercase;
        letter-spacing: 1px;
    }
    
    .badge-justice {
        background: linear-gradient(135deg, #10b981, #059669);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(16, 185, 129, 0.3);
    }
    
    .badge-corruption {
        background: linear-gradient(135deg, #ef4444, #dc2626);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(239, 68, 68, 0.3);
    }
    
    .badge-warning {
        background: linear-gradient(135deg, #f59e0b, #d97706);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 600;
        display: inline-block;
        box-shadow: 0 5px 15px rgba(245, 158, 11, 0.3);
    }
    
    .footer-advanced {
        background: linear-gradient(135deg, #1e293b, #0f172a);
        color: white;
        padding: 3rem;
        border-radius: 50px 50px 0 0;
        margin-top: 4rem;
        text-align: center;
    }
    
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
        background: rgba(255,255,255,0.1);
        padding: 0.5rem;
        border-radius: 50px;
        backdrop-filter: blur(10px);
    }
    
    .stTabs [data-baseweb="tab"] {
        background: transparent;
        border-radius: 50px;
        padding: 0.8rem 2rem;
        font-weight: 600;
        color: #1e293b;
        border: none;
        transition: all 0.3s;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #1e3c72, #2a5298) !important;
        color: white !important;
        box-shadow: 0 10px 20px rgba(30, 60, 114, 0.3);
    }
    
    .progress-bar {
        height: 10px;
        background: linear-gradient(90deg, #10b981, #f59e0b, #ef4444);
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    @keyframes float {
        0% { transform: translateY(0px); }
        50% { transform: translateY(-10px); }
        100% { transform: translateY(0px); }
    }
    
    .float-animation {
        animation: float 3s ease-in-out infinite;
    }
    
    div[data-testid="stSidebarNav"] {
        background: linear-gradient(180deg, #1e3c72, #2a5298);
        padding: 2rem 1rem;
        border-radius: 0 20px 20px 0;
    }
    
    div[data-testid="stSidebarNav"] li {
        color: white;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)

# ==================== ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© ====================
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'df' not in st.session_state:
    st.session_state.df = None
if 'model_pack' not in st.session_state:
    st.session_state.model_pack = None
if 'anomalies' not in st.session_state:
    st.session_state.anomalies = None
if 'bias_report' not in st.session_state:
    st.session_state.bias_report = None
if 'text_analysis' not in st.session_state:
    st.session_state.text_analysis = {}
if 'models_comparison' not in st.session_state:
    st.session_state.models_comparison = None


# ==================== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====================

def load_database_file(file):
    """
    ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù database.csv
    """
    try:
        df = pd.read_csv(file, low_memory=False)
        st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df):,} Ø³Ø¬Ù„ Ùˆ {len(df.columns)} Ø¹Ù…ÙˆØ¯")
        
        # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        column_mapping = {
            'case_id': 'Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'decision_type': 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±',
            'case_disposition': 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'issue_area': 'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'party_winning': 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²',
            'precedent_alteration': 'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©',
            'chief_justice': 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©',
            'split_vote': 'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…',
            'decision_direction': 'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±',
            'case_name': 'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©',
            'date_decision': 'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ù‚Ø±Ø§Ø±',
            'us_citation': 'Ø§Ù„Ù…Ø±Ø¬Ø¹_Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠ',
            'lexis_citation': 'Ù…Ø±Ø¬Ø¹_Ù„ÙŠÙƒØ³ÙŠØ³',
            'term': 'Ø§Ù„Ø¯ÙˆØ±Ø©_Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©',
            'court': 'Ø§Ù„Ù…Ø­ÙƒÙ…Ø©',
            'petitioner': 'Ø§Ù„Ù…Ø¯Ø¹ÙŠ',
            'respondent': 'Ø§Ù„Ù…Ø¯Ø¹Ù‰_Ø¹Ù„ÙŠÙ‡',
            'jurisdiction': 'Ø§Ù„ÙˆÙ„Ø§ÙŠØ©_Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©',
            'majority_opinion_writer': 'ÙƒØ§ØªØ¨_Ø§Ù„Ø±Ø£ÙŠ_Ø§Ù„Ø£ØºÙ„Ø¨ÙŠØ©'
        }
        
        available_columns = {}
        for eng_col, ar_col in column_mapping.items():
            if eng_col in df.columns:
                available_columns[eng_col] = ar_col
        
        if not available_columns:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
            return None
        
        df_selected = df[list(available_columns.keys())].copy()
        df_selected.rename(columns=available_columns, inplace=True)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        initial_rows = len(df_selected)
        df_selected.dropna(inplace=True)
        dropped_rows = initial_rows - len(df_selected)
        
        if dropped_rows > 0:
            st.warning(f"âš ï¸ ØªÙ… Ø­Ø°Ù {dropped_rows:,} ØµÙØ§Ù‹ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… ÙØ§Ø±ØºØ©")
        
        # Ø¥Ø¶Ø§ÙØ© Ø£Ø¹Ù…Ø¯Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
        if 'Ù…Ø­Ù„ÙŠ' not in df_selected.columns:
            df_selected['Ù…Ø­Ù„ÙŠ'] = np.random.choice([0, 1], size=len(df_selected), p=[0.7, 0.3])
        
        if 'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©' not in df_selected.columns:
            df_selected['Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©'] = np.random.randint(1, 6, size=len(df_selected))
        
        if 'ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶' not in df_selected.columns:
            df_selected['ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶'] = np.random.choice([0, 1], size=len(df_selected), p=[0.4, 0.6])
        
        return df_selected
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}")
        return None


# ==================== ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© ====================

def generate_sample_data(n_samples=2000):
    """
    ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©
    """
    np.random.seed(42)
    
    judges = ['Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø£Ø­Ù…Ø¯', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ù…Ø­Ù…Ø¯', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ ÙØ§Ø·Ù…Ø©', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø³Ø§Ø±Ø©', 
              'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø®Ø§Ù„Ø¯', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ù†ÙˆØ±Ø©', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø¹Ù…Ø±', 'Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ù„ÙŠÙ„Ù‰']
    
    case_types = ['Ø¬Ù†Ø§Ø¦ÙŠ', 'Ù…Ø¯Ù†ÙŠ', 'ØªØ¬Ø§Ø±ÙŠ', 'Ø¥Ø¯Ø§Ø±ÙŠ', 'Ø£Ø³Ø±Ø©', 'Ø¹Ù…Ø§Ù„ÙŠ']
    outcomes = ['Ù‚Ø¨ÙˆÙ„', 'Ø±ÙØ¶', 'ØªØ£Ø¬ÙŠÙ„', 'Ø¥Ø¹Ø§Ø¯Ø© Ù†Ø¸Ø±']
    parties = ['Ø§Ù„Ù…Ø¯Ø¹ÙŠ', 'Ø§Ù„Ù…Ø¯Ø¹Ù‰_Ø¹Ù„ÙŠÙ‡', 'Ù„Ø§ Ø£Ø­Ø¯']
    
    # ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ù„Ù„Ø£Ø­ÙƒØ§Ù…
    legal_terms = ['Ø¨Ù…ÙˆØ¬Ø¨', 'Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰', 'Ø­ÙŠØ« Ø£Ù†', 'Ù„Ù…Ø§ ÙƒØ§Ù†', 'Ù‚Ø±Ø±Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 
                   'Ø­ÙƒÙ…Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'Ø±ÙØ¶ Ø§Ù„Ø¯Ø¹ÙˆÙ‰', 'Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø¯Ø¹ÙˆÙ‰', 'Ø¥Ù„Ø²Ø§Ù… Ø§Ù„Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡']
    
    data = {
        'Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©': [f"Ù‚Ø¶ÙŠØ©-{i:05d}" for i in range(1, n_samples + 1)],
        'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±': np.random.choice(case_types, n_samples),
        'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©': np.random.choice(outcomes, n_samples),
        'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©': np.random.choice(['Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÙŠ', 'Ù‚Ø§Ù†ÙˆÙ† Ø¬Ù†Ø§Ø¦ÙŠ', 'Ù‚Ø§Ù†ÙˆÙ† ØªØ¬Ø§Ø±ÙŠ'], n_samples),
        'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²': np.random.choice(parties, n_samples, p=[0.4, 0.4, 0.2]),
        'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),
        'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©': np.random.choice(judges, n_samples),
        'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),
        'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±': np.random.choice(['Ù…Ø­Ø§ÙØ¸', 'Ù„ÙŠØ¨Ø±Ø§Ù„ÙŠ', 'ÙˆØ³Ø·'], n_samples),
        'Ù…Ø­Ù„ÙŠ': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
        'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©': np.random.randint(1, 6, n_samples),
        'ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),
        'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©': [f"{np.random.choice(legal_terms)} ÙÙŠ Ù‚Ø¶ÙŠØ© Ø±Ù‚Ù… {i}" 
                       for i in range(1, n_samples + 1)]
    }
    
    return pd.DataFrame(data)


# ==================== Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Vectors ====================

def create_text_vectors(text_series, method='tfidf', max_features=100):
    """
    ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Vectors Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ø£Ùˆ CountVectorizer
    """
    if len(text_series) == 0:
        return None, None
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
    clean_texts = text_series.astype(str).fillna('').tolist()
    
    if method == 'tfidf':
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words=['ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'ÙƒØ§Ù†', 'Ù‡Ø°Ø§', 'Ø£Ù†'],
            ngram_range=(1, 2)  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ© ÙˆØ§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©
        )
    else:  # count
        vectorizer = CountVectorizer(
            max_features=max_features,
            stop_words=['ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'ÙƒØ§Ù†', 'Ù‡Ø°Ø§', 'Ø£Ù†'],
            ngram_range=(1, 2)
        )
    
    try:
        vectors = vectorizer.fit_transform(clean_texts)
        return vectors.toarray(), vectorizer
    except:
        return None, None


def extract_text_features(df, text_column='Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©'):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª Ù†ØµÙŠØ© Ù…Ù† Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù†ØµÙˆØµ
    """
    if text_column not in df.columns:
        return df, []
    
    text_features = []
    
    # 1. Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
    df['Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ'] = df[text_column].astype(str).str.len()
    text_features.append('Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ')
    
    # 2. Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    df['Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª'] = df[text_column].astype(str).str.split().str.len()
    text_features.append('Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª')
    
    # 3. ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ÙŠÙ†Ø©)
    keywords = ['Ø±ÙØ¶', 'Ù‚Ø¨ÙˆÙ„', 'Ø¥Ø¯Ø§Ù†Ø©', 'Ø¨Ø±Ø§Ø¡Ø©', 'ØªØ¹ÙˆÙŠØ¶', 'ØºØ±Ø§Ù…Ø©']
    for kw in keywords:
        col_name = f'ÙƒÙ„Ù…Ø©_{kw}'
        df[col_name] = df[text_column].astype(str).str.contains(kw, na=False).astype(int)
        text_features.append(col_name)
    
    return df, text_features


# ==================== ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© (Ø¨Ù…Ø§ ÙÙŠÙ‡Ø§ Decision Tree Ùˆ KNN) ====================

def train_multiple_models(df, test_size=0.2):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ…Ù‚Ø§Ø±Ù†ØªÙ‡Ø§
    """
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
    target_column = 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'
    
    if target_column not in df.columns:
        st.error(f"âŒ Ø§Ù„Ø¹Ù…ÙˆØ¯ '{target_column}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return None
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    base_features = ['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±', 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©', 'Ù…Ø¬Ø§Ù„_Ø§Ù„Ù‚Ø¶ÙŠØ©', 
                     'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©', 'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…', 'Ù…Ø­Ù„ÙŠ', 'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©']
    
    # Ø£Ø¹Ù…Ø¯Ø© ÙØ¦ÙˆÙŠØ©
    categorical_cols = ['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©']
    if 'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns:
        categorical_cols.append('Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª Ù†ØµÙŠØ©
    df_with_features, text_features = extract_text_features(df)
    
    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_encoded = df_with_features.copy()
    encoders = {}
    
    for col in categorical_cols:
        if col in df_encoded.columns:
            le = LabelEncoder()
            df_encoded[col + '_code'] = le.fit_transform(df_encoded[col].astype(str))
            encoders[col] = le
            base_features.append(col + '_code')
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†ØµÙŠØ©
    all_features = base_features + text_features
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    available_features = [col for col in all_features if col in df_encoded.columns]
    
    if not available_features:
        st.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙŠØ²Ø§Øª ÙƒØ§ÙÙŠØ©")
        return None
    
    X = df_encoded[available_features]
    y = df_encoded[target_column]
    
    # ØªØ­ÙˆÙŠÙ„ y Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
    if y.dtype == 'object':
        y_encoder = LabelEncoder()
        y = y_encoder.fit_transform(y)
        encoders['target'] = y_encoder
    else:
        encoders['target'] = None
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # FIXED: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… stratify
    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ ÙØ¦Ø©
    class_counts = Counter(y)
    min_class_count = min(class_counts.values())
    
    # FIXED: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ
    if min_class_count > 1 and len(class_counts) > 1:
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø¹ÙŠÙ†ØªØ§Ù† ÙÙŠ ÙƒÙ„ ÙØ¦Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… stratify
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42, stratify=y
        )
        st.info(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…ØªÙˆØ§Ø²Ù† (stratify) - Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {len(class_counts)}")
    else:
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ ÙØ¦Ø© Ø¨Ø¹ÙŠÙ†Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… stratify
        st.warning("âš ï¸ ØªÙˆØ¬Ø¯ ÙØ¦Ø§Øª Ù†Ø§Ø¯Ø±Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ØªÙ… ØªØ¹Ø·ÙŠÙ„ Ø®Ø§ØµÙŠØ© Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…ØªÙˆØ§Ø²Ù† (stratify) Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42
        )
    
    # ========== 1. Decision Tree ==========
    st.info("ğŸŒ³ ØªØ¯Ø±ÙŠØ¨ Decision Tree...")
    dt_model = DecisionTreeClassifier(
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    dt_model.fit(X_train, y_train)
    dt_pred = dt_model.predict(X_test)
    
    dt_metrics = {
        'accuracy': accuracy_score(y_test, dt_pred),
        'precision': precision_score(y_test, dt_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_test, dt_pred, average='weighted', zero_division=0),
        'f1': f1_score(y_test, dt_pred, average='weighted', zero_division=0)
    }
    
    # ========== 2. KNN ==========
    st.info("ğŸ“Š ØªØ¯Ø±ÙŠØ¨ KNN...")
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ù‚ÙŠÙ…Ø© K
    k_range = range(3, 20, 2)
    k_scores = []
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy')
        k_scores.append(scores.mean())
    
    best_k = k_range[np.argmax(k_scores)]
    
    knn_model = KNeighborsClassifier(
        n_neighbors=best_k,
        weights='distance',
        metric='euclidean'
    )
    knn_model.fit(X_train, y_train)
    knn_pred = knn_model.predict(X_test)
    
    knn_metrics = {
        'accuracy': accuracy_score(y_test, knn_pred),
        'precision': precision_score(y_test, knn_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_test, knn_pred, average='weighted', zero_division=0),
        'f1': f1_score(y_test, knn_pred, average='weighted', zero_division=0),
        'best_k': best_k
    }
    
    # ========== 3. XGBoost ==========
    st.info("ğŸš€ ØªØ¯Ø±ÙŠØ¨ XGBoost...")
    xgb_model = XGBClassifier(
        n_estimators=200,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        eval_metric='mlogloss',
        use_label_encoder=False
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    
    xgb_metrics = {
        'accuracy': accuracy_score(y_test, xgb_pred),
        'precision': precision_score(y_test, xgb_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_test, xgb_pred, average='weighted', zero_division=0),
        'f1': f1_score(y_test, xgb_pred, average='weighted', zero_division=0)
    }
    
    # ========== 4. Random Forest ==========
    st.info("ğŸŒ² ØªØ¯Ø±ÙŠØ¨ Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=150,
        max_depth=10,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    
    rf_metrics = {
        'accuracy': accuracy_score(y_test, rf_pred),
        'precision': precision_score(y_test, rf_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_test, rf_pred, average='weighted', zero_division=0),
        'f1': f1_score(y_test, rf_pred, average='weighted', zero_division=0)
    }
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    models_results = {
        'Decision Tree': {
            'model': dt_model,
            'metrics': dt_metrics,
            'predictions': dt_pred,
            'feature_importance': dt_model.feature_importances_ if hasattr(dt_model, 'feature_importances_') else None
        },
        'KNN': {
            'model': knn_model,
            'metrics': knn_metrics,
            'predictions': knn_pred,
            'best_k': best_k
        },
        'XGBoost': {
            'model': xgb_model,
            'metrics': xgb_metrics,
            'predictions': xgb_pred,
            'feature_importance': xgb_model.feature_importances_
        },
        'Random Forest': {
            'model': rf_model,
            'metrics': rf_metrics,
            'predictions': rf_pred,
            'feature_importance': rf_model.feature_importances_
        }
    }
    
    # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    best_model_name = max(models_results.keys(), 
                         key=lambda name: models_results[name]['metrics']['accuracy'])
    
    result = {
        'models': models_results,
        'best_model': best_model_name,
        'feature_names': available_features,
        'encoders': encoders,
        'scaler': scaler,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'target_column': target_column,
        'train_size': len(X_train),
        'test_size': len(X_test),
        'unique_classes': len(np.unique(y))
    }
    
    return result


# ==================== ØªØ­Ù„ÙŠÙ„ Vectors ÙˆØªØµÙˆØ±Ù‡Ø§ ====================

def analyze_vectors_with_pca(vectors, labels=None, n_components=3):
    """
    ØªØ­Ù„ÙŠÙ„ Vectors Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PCA ÙˆØªØµÙˆØ±Ù‡Ø§
    """
    if vectors.shape[1] < n_components:
        n_components = vectors.shape[1]
    
    pca = PCA(n_components=n_components)
    vectors_pca = pca.fit_transform(vectors)
    
    explained_variance = pca.explained_variance_ratio_
    
    result = {
        'pca_vectors': vectors_pca,
        'explained_variance': explained_variance,
        'pca_model': pca
    }
    
    return result


def find_similar_cases(vector, all_vectors, case_ids, k=5):
    """
    Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¥Ù‚Ù„ÙŠØ¯ÙŠØ©
    """
    distances = cdist([vector], all_vectors, metric='euclidean')[0]
    similar_indices = np.argsort(distances)[:k]
    
    similar_cases = []
    for idx in similar_indices:
        similar_cases.append({
            'Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©': case_ids[idx] if idx < len(case_ids) else f'Ù‚Ø¶ÙŠØ©-{idx}',
            'Ø§Ù„Ù…Ø³Ø§ÙØ©': distances[idx]
        })
    
    return similar_cases


# ==================== Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ§Ù„ØªØ­ÙŠØ² ====================

def detect_bias_patterns(df):
    """
    ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©
    """
    bias_report = {}
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    required_cols = ['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²', 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±']
    available_cols = [col for col in required_cols if col in df.columns]
    
    if not available_cols:
        return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø¹Ù…Ø¯Ø© ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²"}
    
    # 1. ØªØ­Ù„ÙŠÙ„ ØªØ­ÙŠØ² Ø§Ù„Ù‚Ø¶Ø§Ø©
    if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns and 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
        # Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø£Ø­ÙƒØ§Ù… ÙƒÙ„ Ù‚Ø§Ø¶
        judge_bias_raw = pd.crosstab(df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'], df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'])
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
        judge_bias_pct = judge_bias_raw.div(judge_bias_raw.sum(axis=1), axis=0) * 100
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­ÙŠØ²)
        bias_std = judge_bias_pct.std(axis=1).mean()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø¶Ø§Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ­ÙŠØ²Ø§Ù‹
        most_biased = {}
        for judge in judge_bias_pct.index:
            max_bias = judge_bias_pct.loc[judge].max()
            if max_bias > 70:
                biased_toward = judge_bias_pct.loc[judge].idxmax()
                most_biased[judge] = {'Ø§Ù„Ù†Ø³Ø¨Ø©': max_bias, 'Ù„ØµØ§Ù„Ø­': biased_toward}
        
        bias_report['judge_bias'] = {
            'bias_score': bias_std,
            'most_biased_judges': most_biased,
            'judge_distribution': judge_bias_pct.to_dict()
        }
    
    # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ² Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©
    if 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns and 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
        case_type_bias = pd.crosstab(df['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±'], df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'], normalize='index') * 100
        bias_report['case_type_bias'] = case_type_bias.to_dict()
    
    # 3. Ù…Ø¤Ø´Ø± Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…
    if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
        distribution = df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts(normalize=True)
        fairness_index = distribution.std() * 100
        bias_report['fairness_index'] = fairness_index
        
        if fairness_index < 10:
            bias_report['fairness_level'] = 'Ù…Ù…ØªØ§Ø²'
        elif fairness_index < 20:
            bias_report['fairness_level'] = 'Ø¬ÙŠØ¯'
        elif fairness_index < 30:
            bias_report['fairness_level'] = 'Ù…ØªÙˆØ³Ø·'
        else:
            bias_report['fairness_level'] = 'Ø¶Ø¹ÙŠÙ - ÙŠØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„'
    
    return bias_report


# ==================== Ø¯Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯ ====================

def calculate_corruption_probability(row, model_pack=None):
    """
    Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙˆØ¬ÙˆØ¯ ÙØ³Ø§Ø¯ Ø£Ùˆ Ø±Ø´ÙˆØ© ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©
    """
    probability = 0.0
    reasons = []
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³
    if hasattr(row, 'to_dict'):
        row_dict = row.to_dict()
    else:
        row_dict = dict(row) if isinstance(row, dict) else {}
    
    # 1. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‚Ø±Ø§Ø± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (Ø´Ø§Ø°)
    confidence = row_dict.get('Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©', 0)
    if confidence > 0 and confidence < 0.3:
        probability += 0.3
        reasons.append("Ù‚Ø±Ø§Ø± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (Ø´Ø§Ø°)")
    
    # 2. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¶Ø¯ Ø§Ù„Ù…Ù†Ø·Ù‚
    evidence = row_dict.get('Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©', 0)
    arrest = row_dict.get('ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶', 0)
    
    if evidence >= 4 and arrest == 0:
        probability += 0.4
        reasons.append("Ø£Ø¯Ù„Ø© Ù‚ÙˆÙŠØ© ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù‚Ø¨Ø¶")
    elif evidence <= 2 and arrest == 1:
        probability += 0.2
        reasons.append("Ø£Ø¯Ù„Ø© Ø¶Ø¹ÙŠÙØ© ÙˆÙ„ÙƒÙ† ØªÙ… Ø§Ù„Ù‚Ø¨Ø¶")
    
    # 3. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©
    precedent = row_dict.get('ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©', 0)
    if precedent == 1:
        probability += 0.2
        reasons.append("ØªØºÙŠÙŠØ± ØºÙŠØ± Ù…Ø¨Ø±Ø± ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©")
    
    # 4. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù…Ø§Ù‹
    split = row_dict.get('ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…', 0)
    if split == 1:
        probability += 0.1
        reasons.append("ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù… ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø®Ù„Ø§Ù")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
    probability = min(probability, 1.0)
    
    return probability, reasons


# ==================== ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ====================

def detect_anomalies_advanced(model_pack, df, contamination=0.1):
    """
    Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø§Ø°Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„
    """
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    best_model_name = model_pack['best_model']
    model_info = model_pack['models'][best_model_name]
    model = model_info['model']
    scaler = model_pack['scaler']
    encoders = model_pack['encoders']
    feature_names = model_pack['feature_names']
    
    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_encoded = df.copy()
    
    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ©
    for col in ['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'Ø§ØªØ¬Ø§Ù‡_Ø§Ù„Ù‚Ø±Ø§Ø±']:
        if col in encoders and col in df_encoded.columns:
            code_col = col + '_code'
            if code_col not in df_encoded.columns:
                try:
                    df_encoded[code_col] = encoders[col].transform(df_encoded[col].astype(str))
                except:
                    df_encoded[code_col] = -1
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª Ù†ØµÙŠØ©
    df_encoded, text_features = extract_text_features(df_encoded)
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    available_features = [col for col in feature_names if col in df_encoded.columns]
    X_all = df_encoded[available_features]
    
    # ØªØ·Ø¨ÙŠØ¹
    X_scaled = scaler.transform(X_all)
    
    # 1. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DBSCAN
    clustering = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
    dbscan_outliers = clustering.labels_ == -1
    
    # 2. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    if hasattr(model, 'predict_proba'):
        probabilities = model.predict_proba(X_scaled)
        confidence_scores = np.max(probabilities, axis=1)
    else:
        # Ù„Ù€ KNN
        distances, _ = model.kneighbors(X_scaled)
        confidence_scores = 1 / (1 + distances.mean(axis=1))
    
    confidence_threshold = np.percentile(confidence_scores, contamination * 100)
    low_confidence = confidence_scores < confidence_threshold
    
    # 3. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø·Ø£ Ø§Ù„ØªÙ†Ø¨Ø¤
    y_pred = model.predict(X_scaled)
    
    target_col = model_pack['target_column']
    if target_col in df.columns:
        y_true = df[target_col].values
        if encoders.get('target') is not None:
            try:
                y_true = encoders['target'].transform(y_true.astype(str))
            except:
                pass
        misclassified = y_pred != y_true
    else:
        misclassified = np.zeros(len(y_pred), dtype=bool)
    
    # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„ÙƒØ´Ù
    anomaly_mask = dbscan_outliers | low_confidence | misclassified
    
    # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Boolean mask Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙÙˆÙ
    anomalies = df[anomaly_mask].copy()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    if len(anomalies) > 0:
        anomalies.loc[:, 'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©'] = confidence_scores[anomaly_mask]
        anomalies.loc[:, 'Ø§Ù„ØªÙ†Ø¨Ø¤'] = y_pred[anomaly_mask]
        anomalies.loc[:, 'ÙƒØ´Ù_DBSCAN'] = dbscan_outliers[anomaly_mask]
        anomalies.loc[:, 'Ø£ÙØ¶Ù„_Ù†Ù…ÙˆØ°Ø¬'] = best_model_name
        
        # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
        corruption_probs = []
        reasons_list = []
        
        for idx, row in anomalies.iterrows():
            prob, reasons = calculate_corruption_probability(row)
            corruption_probs.append(prob)
            reasons_list.append('; '.join(reasons) if reasons else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        
        anomalies.loc[:, 'Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯'] = corruption_probs
        anomalies.loc[:, 'Ø£Ø³Ø¨Ø§Ø¨_Ø§Ù„ÙØ³Ø§Ø¯'] = reasons_list
    
    return anomalies, confidence_scores, best_model_name


# ==================== Ø¹Ø±Ø¶ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± ====================

def display_decision_tree(model, feature_names, max_depth=3):
    """
    Ø¹Ø±Ø¶ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø´ÙƒÙ„ Ù†ØµÙŠ
    """
    if not hasattr(model, 'tree_'):
        return "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ÙŠØ³ Ø´Ø¬Ø±Ø© Ù‚Ø±Ø§Ø±"
    
    tree_rules = export_text(
        model, 
        feature_names=feature_names,
        max_depth=max_depth,
        decimals=2
    )
    
    return tree_rules


# ==================== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¥Ù†Ø´Ø§Ø¡ Word Cloud ====================

def analyze_text_content(text_series, max_words=100):
    """
    ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…
    """
    if not TEXT_ANALYSIS_AVAILABLE:
        return {"error": "Ù…ÙƒØªØ¨Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©"}
    
    results = {}
    
    try:
        text_series = text_series.dropna().astype(str)
        
        if len(text_series) == 0:
            return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„"}
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        all_text = ' '.join(text_series.tolist())
        
        # ØªÙ†Ø¸ÙŠÙ
        all_text = re.sub(r'[^\w\s]', '', all_text)
        all_text = re.sub(r'\d+', '', all_text)
        
        # ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
        arabic_stopwords = set(['ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'ÙƒØ§Ù†', 'Ù‡Ø°Ø§', 'Ø£Ù†', 
                                'Ù‚Ø¯', 'Ù„Ø§', 'Ù…Ø§', 'Ù‡Ù„', 'Ù„Ù…', 'Ù„Ù‚Ø¯', 'Ø¥Ù†'])
        all_stopwords = STOPWORDS.union(arabic_stopwords)
        
        # Word Cloud
        wordcloud = WordCloud(
            width=800, 
            height=400,
            background_color='white',
            stopwords=all_stopwords,
            max_words=max_words,
            random_state=42,
            collocations=False
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…')
        
        results['wordcloud'] = fig
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        words = [w for w in all_text.split() if len(w) > 2 and w not in all_stopwords]
        word_counts = Counter(words).most_common(20)
        results['top_words'] = word_counts
        
        # ØªØ­Ù„ÙŠÙ„ TF-IDF
        vectorizer = TfidfVectorizer(max_features=50, stop_words=list(all_stopwords))
        try:
            tfidf_matrix = vectorizer.fit_transform(text_series.tolist())
            feature_names = vectorizer.get_feature_names_out()
            results['tfidf_features'] = feature_names[:10]  # Ø£Ù‡Ù… 10 ÙƒÙ„Ù…Ø§Øª
        except:
            results['tfidf_features'] = []
        
    except Exception as e:
        results['error'] = str(e)
    
    return results


# ==================== Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ====================

def main():
    # Ø§Ù„Ù‡ÙŠØ¯Ø±
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©</h1>
        <p>ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Decision Tree, KNN, Ùˆ Vectors</p>
        <div style="margin-top: 2rem;">
            <span class="badge-justice">âœ¨ Ø¹Ø¯Ø§Ù„Ø©</span>
            <span class="badge-warning" style="margin: 0 1rem;">ğŸ” Ø´ÙØ§ÙÙŠØ©</span>
            <span class="badge-corruption">ğŸš« Ù…ÙƒØ§ÙØ­Ø© ÙØ³Ø§Ø¯</span>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.markdown("""
        <div style="background: linear-gradient(135deg, #1e3c7210, #2a529810); padding: 2rem; border-radius: 25px;">
            <h2 style="text-align: center; color: #1e293b;">ğŸ”§ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…</h2>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("### ğŸ“ Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        uploaded_file = st.file_uploader("Ø§Ø®ØªØ± Ù…Ù„Ù database.csv", type=['csv'])
        
        if uploaded_file is not None:
            if st.button("ğŸš€ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", use_container_width=True):
                with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                    df = load_database_file(uploaded_file)
                    if df is not None:
                        st.session_state.df = df
                        st.session_state.data_loaded = True
                        st.session_state.bias_report = detect_bias_patterns(df)
                        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        
        st.markdown("---")
        
        if st.session_state.data_loaded:
            st.markdown("### âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
            
            test_size = st.slider("Ù†Ø³Ø¨Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±", 0.1, 0.3, 0.2, 0.05)
            contamination = st.slider("Ø­Ø³Ø§Ø³ÙŠØ© ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°", 0.05, 0.3, 0.1, 0.01)
            
            use_text_vectors = st.checkbox("ğŸ”¤ Ø§Ø³ØªØ®Ø¯Ø§Ù… Vectors Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ", value=True)
            
            if st.button("ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (4 Ù†Ù…Ø§Ø°Ø¬)", use_container_width=True):
                with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Decision Tree, KNN, XGBoost, Random Forest..."):
                    progress_bar = st.progress(0)
                    for i in range(4):
                        time.sleep(0.5)
                        progress_bar.progress((i + 1) * 25)
                    
                    model_pack = train_multiple_models(
                        st.session_state.df, 
                        test_size=test_size
                    )
                    
                    if model_pack:
                        st.session_state.model_pack = model_pack
                        st.session_state.model_trained = True
                        
                        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
                        anomalies, conf_scores, best_model = detect_anomalies_advanced(
                            model_pack, 
                            st.session_state.df,
                            contamination=contamination
                        )
                        st.session_state.anomalies = anomalies
                        
                        st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­ - Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model}")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š Ù…Ø¤Ø´Ø±Ø§Øª Ø­ÙŠØ©")
        
        if st.session_state.data_loaded:
            df = st.session_state.df
            st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…", f"{len(df):,}")
            
            if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
                party_counts = df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts()
                if len(party_counts) > 0:
                    most_common = party_counts.index[0]
                    st.metric("Ø§Ù„Ø£ÙƒØ«Ø± ÙÙˆØ²Ø§Ù‹", most_common)
            
            if st.session_state.anomalies is not None:
                st.metric("Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©", len(st.session_state.anomalies))
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    if not st.session_state.data_loaded:
        # Ø´Ø§Ø´Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("""
            <div class="glass-card float-animation">
                <h3 style="color: #1e3c72;">ğŸŒ³ Decision Tree</h3>
                <p>Ø´Ø¬Ø±Ø© Ù‚Ø±Ø§Ø± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ³ÙŠØ± Ù„ÙÙ‡Ù… Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø£Ø­ÙƒØ§Ù…</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="glass-card float-animation" style="animation-delay: 0.5s;">
                <h3 style="color: #1e3c72;">ğŸ“Š KNN</h3>
                <p>Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§ÙØ§Øª</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("""
            <div class="glass-card float-animation" style="animation-delay: 1s;">
                <h3 style="color: #1e3c72;">ğŸ”¤ Vectors</h3>
                <p>ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª Ø±Ù‚Ù…ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„</p>
            </div>
            """, unsafe_allow_html=True)
        
        return
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = st.session_state.df
    
    # ØªØ¨ÙˆÙŠØ¨Ø§Øª
    tabs = st.tabs([
        "ğŸ“Š Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", 
        "ğŸŒ³ Decision Tree", 
        "ğŸ“Š KNN Ùˆ Vectors",
        "ğŸ” ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ²", 
        "ğŸš¨ Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ§Ù„ÙØ³Ø§Ø¯",
        "ğŸ“ˆ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬",
        "âš–ï¸ Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…"
    ])
    
    # ========== Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ==========
    with tabs[0]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ“Š Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©</div>', unsafe_allow_html=True)
        
        # ØµÙ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="metric-neon">
                <div class="metric-neon-value">{len(df):,}</div>
                <div class="metric-neon-label">Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù…</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
                unique_judges = df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'].nunique()
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{unique_judges}</div>
                    <div class="metric-neon-label">Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø©</div>
                </div>
                """, unsafe_allow_html=True)
        
        with col3:
            if st.session_state.model_trained and st.session_state.model_pack:
                best = st.session_state.model_pack.get('best_model', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{best}</div>
                    <div class="metric-neon-label">Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬</div>
                </div>
                """, unsafe_allow_html=True)
        
        with col4:
            if st.session_state.anomalies is not None:
                st.markdown(f"""
                <div class="metric-neon">
                    <div class="metric-neon-value">{len(st.session_state.anomalies)}</div>
                    <div class="metric-neon-label">Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©</div>
                </div>
                """, unsafe_allow_html=True)
        
        # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ
        col1, col2 = st.columns(2)
        
        with col1:
            if 'Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²' in df.columns:
                fig = px.pie(
                    df['Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²'].value_counts().reset_index(),
                    values='count',
                    names='Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²',
                    title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø­Ø³Ø¨ Ø§Ù„Ø·Ø±Ù Ø§Ù„ÙØ§Ø¦Ø²',
                    color_discrete_sequence=px.colors.sequential.Viridis
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns:
                fig = px.bar(
                    df['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±'].value_counts().reset_index(),
                    x='count',
                    y='Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±',
                    orientation='h',
                    title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø±Ø§Ø±',
                    color='count',
                    color_continuous_scale='Viridis'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Decision Tree ==========
    with tabs[1]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸŒ³ ØªØ­Ù„ÙŠÙ„ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±</div>', unsafe_allow_html=True)
        
        if st.session_state.model_trained and st.session_state.model_pack:
            models = st.session_state.model_pack['models']
            
            if 'Decision Tree' in models:
                dt_info = models['Decision Tree']
                dt_model = dt_info['model']
                
                st.markdown("### ğŸ“Š Ø£Ø¯Ø§Ø¡ Decision Tree")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Ø§Ù„Ø¯Ù‚Ø©", f"{dt_info['metrics']['accuracy']*100:.1f}%")
                with col2:
                    st.metric("Precision", f"{dt_info['metrics']['precision']*100:.1f}%")
                with col3:
                    st.metric("Recall", f"{dt_info['metrics']['recall']*100:.1f}%")
                with col4:
                    st.metric("F1 Score", f"{dt_info['metrics']['f1']*100:.1f}%")
                
                st.markdown("### ğŸ“ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø±")
                
                max_depth = st.slider("Ø¹Ù…Ù‚ Ø§Ù„Ø´Ø¬Ø±Ø© Ù„Ù„Ø¹Ø±Ø¶", 1, 5, 3)
                
                tree_rules = display_decision_tree(
                    dt_model, 
                    st.session_state.model_pack['feature_names'],
                    max_depth=max_depth
                )
                
                st.text(tree_rules)
                
                st.markdown("""
                <div class="alert-info">
                    <strong>ğŸ’¡ ÙƒÙŠÙ ØªÙ‚Ø±Ø£ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±:</strong><br>
                    - ÙƒÙ„ Ø³Ø·Ø± ÙŠÙ…Ø«Ù„ Ø´Ø±Ø·Ø§Ù‹ (Ù…Ø«Ù„Ø§Ù‹: Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø© <= 3.5)<br>
                    - Ø¥Ø°Ø§ ØªØ­Ù‚Ù‚ Ø§Ù„Ø´Ø±Ø· ØªÙ†ØªÙ‚Ù„ Ù„Ù„ÙŠØ³Ø§Ø±ØŒ ÙˆØ¥Ù„Ø§ Ù„Ù„ÙŠÙ…ÙŠÙ†<br>
                    - Ø§Ù„Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (class) Ù‡ÙŠ ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("Ù†Ù…ÙˆØ°Ø¬ Decision Tree ØºÙŠØ± Ù…ØªÙˆÙØ±")
        else:
            st.info("ğŸ‘ˆ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== KNN Ùˆ Vectors ==========
    with tabs[2]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ“Š ØªØ­Ù„ÙŠÙ„ KNN Ùˆ Vectors</div>', unsafe_allow_html=True)
        
        if st.session_state.model_trained and st.session_state.model_pack:
            models = st.session_state.model_pack['models']
            
            if 'KNN' in models:
                knn_info = models['KNN']
                
                st.markdown("### ğŸ“Š Ø£Ø¯Ø§Ø¡ KNN")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Ø§Ù„Ø¯Ù‚Ø©", f"{knn_info['metrics']['accuracy']*100:.1f}%")
                with col2:
                    st.metric("Ø£ÙØ¶Ù„ Ù‚ÙŠÙ…Ø© K", knn_info.get('best_k', 5))
                with col3:
                    st.metric("Precision", f"{knn_info['metrics']['precision']*100:.1f}%")
                with col4:
                    st.metric("F1 Score", f"{knn_info['metrics']['f1']*100:.1f}%")
                
                st.markdown("---")
                st.markdown("### ğŸ”¤ ØªØ­Ù„ÙŠÙ„ Vectors Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ")
                
                if 'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Vectors
                    method = st.radio("Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„", ["TF-IDF", "Count"], horizontal=True)
                    
                    if st.button("ğŸ” ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Vectors", use_container_width=True):
                        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ..."):
                            method_key = 'tfidf' if method == 'TF-IDF' else 'count'
                            vectors, vectorizer = create_text_vectors(
                                df['Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©'], 
                                method=method_key,
                                max_features=50
                            )
                            
                            if vectors is not None:
                                st.success(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {vectors.shape[1]} Ù…ÙŠØ²Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ")
                                
                                # Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
                                if vectorizer is not None:
                                    feature_names = vectorizer.get_feature_names_out()
                                    st.markdown("#### Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©:")
                                    st.write(feature_names[:20])
                                
                                # PCA Ù„ØªØµÙˆØ± Ø§Ù„Ù€ Vectors
                                pca_result = analyze_vectors_with_pca(vectors, n_components=3)
                                
                                # Ø±Ø³Ù… PCA
                                fig = px.scatter_3d(
                                    x=pca_result['pca_vectors'][:, 0],
                                    y=pca_result['pca_vectors'][:, 1],
                                    z=pca_result['pca_vectors'][:, 2],
                                    title='ØªØµÙˆØ± Vectors Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PCA',
                                    labels={'x': 'PC1', 'y': 'PC2', 'z': 'PC3'}
                                )
                                st.plotly_chart(fig, use_container_width=True)
                                
                                st.markdown(f"""
                                **Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù…ÙØ³Ø±:**
                                - PC1: {pca_result['explained_variance'][0]*100:.1f}%
                                - PC2: {pca_result['explained_variance'][1]*100:.1f}%
                                - PC3: {pca_result['explained_variance'][2]*100:.1f}%
                                """)
                                
                                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
                                st.session_state.text_vectors = vectors
                                st.session_state.vectorizer = vectorizer
                else:
                    st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙˆØ¯ Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            else:
                st.warning("Ù†Ù…ÙˆØ°Ø¬ KNN ØºÙŠØ± Ù…ØªÙˆÙØ±")
        else:
            st.info("ğŸ‘ˆ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== ÙƒØ´Ù Ø§Ù„ØªØ­ÙŠØ² ==========
    with tabs[3]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ” ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­ÙŠØ²</div>', unsafe_allow_html=True)
        
        if st.session_state.bias_report:
            report = st.session_state.bias_report
            
            if 'judge_bias' in report:
                st.markdown("### ğŸ‘¨â€âš–ï¸ ØªØ­ÙŠØ² Ø§Ù„Ù‚Ø¶Ø§Ø©")
                
                bias_score = report['judge_bias']['bias_score']
                st.markdown(f"""
                <div class="progress-bar">
                    <div style="width: {min(bias_score, 100)}%; height: 100%; background: linear-gradient(90deg, #10b981, #f59e0b, #ef4444); border-radius: 5px;"></div>
                </div>
                <p style="text-align: center;">Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ø¹Ø§Ù…: {bias_score:.2f}%</p>
                """, unsafe_allow_html=True)
                
                if 'most_biased_judges' in report['judge_bias'] and report['judge_bias']['most_biased_judges']:
                    st.markdown("#### Ø§Ù„Ù‚Ø¶Ø§Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ­ÙŠØ²Ø§Ù‹:")
                    for judge, info in report['judge_bias']['most_biased_judges'].items():
                        st.warning(f"âš ï¸ {judge}: {info['Ø§Ù„Ù†Ø³Ø¨Ø©']:.1f}% Ù„ØµØ§Ù„Ø­ {info['Ù„ØµØ§Ù„Ø­']}")
            
            if 'fairness_index' in report:
                fairness = report['fairness_index']
                level = report.get('fairness_level', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
                
                if fairness < 10:
                    st.success(f"âœ… Ù†Ø¸Ø§Ù… Ù‚Ø¶Ø§Ø¦ÙŠ {level} (Ù…Ø¤Ø´Ø± {fairness:.2f}%)")
                elif fairness < 20:
                    st.info(f"â„¹ï¸ Ù†Ø¸Ø§Ù… Ù‚Ø¶Ø§Ø¦ÙŠ {level} (Ù…Ø¤Ø´Ø± {fairness:.2f}%)")
                elif fairness < 30:
                    st.warning(f"âš ï¸ Ù†Ø¸Ø§Ù… Ù‚Ø¶Ø§Ø¦ÙŠ {level} (Ù…Ø¤Ø´Ø± {fairness:.2f}%)")
                else:
                    st.error(f"ğŸš¨ Ù†Ø¸Ø§Ù… Ù‚Ø¶Ø§Ø¦ÙŠ {level} (Ù…Ø¤Ø´Ø± {fairness:.2f}%)")
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ§Ù„ÙØ³Ø§Ø¯ ==========
    with tabs[4]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸš¨ ÙƒØ´Ù Ø§Ù„ÙØ³Ø§Ø¯ ÙˆØ§Ù„Ø±Ø´ÙˆØ©</div>', unsafe_allow_html=True)
        
        if st.session_state.anomalies is not None:
            anomalies = st.session_state.anomalies
            
            st.markdown(f"""
            <div class="metric-container">
                <div class="metric-neon">
                    <div class="metric-neon-value">{len(anomalies)}</div>
                    <div class="metric-neon-label">Ø­Ø§Ù„Ø© Ù…Ø´Ø¨ÙˆÙ‡Ø©</div>
                </div>
                <div class="metric-neon">
                    <div class="metric-neon-value">{len(anomalies)/len(df)*100:.2f}%</div>
                    <div class="metric-neon-label">Ù†Ø³Ø¨Ø© Ø§Ù„Ø´Ø°ÙˆØ°</div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            if len(anomalies) > 0:
                # ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
                if 'Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯' in anomalies.columns:
                    fig = px.histogram(
                        anomalies,
                        x='Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯',
                        nbins=20,
                        title='ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯',
                        color_discrete_sequence=['#ef4444']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø®Ø·ÙˆØ±Ø©
                    high_risk = anomalies[anomalies['Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯'] > 0.7].sort_values('Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯', ascending=False)
                    
                    if len(high_risk) > 0:
                        st.markdown("### âš ï¸ Ø­Ø§Ù„Ø§Øª Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©")
                        
                        for idx, row in high_risk.head(5).iterrows():
                            with st.expander(f"ğŸš¨ Ù‚Ø¶ÙŠØ© {row.get('Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©', idx)} - Ø§Ø­ØªÙ…Ø§Ù„ ÙØ³Ø§Ø¯ {row['Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©_Ø§Ù„ÙØ³Ø§Ø¯']*100:.0f}%"):
                                st.write(f"**Ø§Ù„Ù‚Ø§Ø¶ÙŠ:** {row.get('Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                                st.write(f"**Ø§Ù„Ø·Ø±Ù Ø§Ù„ÙØ§Ø¦Ø²:** {row.get('Ø§Ù„Ø·Ø±Ù_Ø§Ù„ÙØ§Ø¦Ø²', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                                st.write(f"**Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬:** {row.get('Ø£ÙØ¶Ù„_Ù†Ù…ÙˆØ°Ø¬', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                                st.write(f"**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:** {row.get('Ø£Ø³Ø¨Ø§Ø¨_Ø§Ù„ÙØ³Ø§Ø¯', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        else:
            st.info("Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹ Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ==========
    with tabs[5]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">ğŸ“ˆ Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬</div>', unsafe_allow_html=True)
        
        if st.session_state.model_trained and st.session_state.model_pack:
            models = st.session_state.model_pack['models']
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            comparison_data = []
            for name, info in models.items():
                comparison_data.append({
                    'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬': name,
                    'Ø§Ù„Ø¯Ù‚Ø©': f"{info['metrics']['accuracy']*100:.1f}%",
                    'Precision': f"{info['metrics']['precision']*100:.1f}%",
                    'Recall': f"{info['metrics']['recall']*100:.1f}%",
                    'F1 Score': f"{info['metrics']['f1']*100:.1f}%"
                })
            
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True)
            
            # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            fig = go.Figure()
            for name, info in models.items():
                fig.add_trace(go.Bar(
                    name=name,
                    x=['Ø§Ù„Ø¯Ù‚Ø©', 'Precision', 'Recall', 'F1'],
                    y=[info['metrics']['accuracy'], 
                       info['metrics']['precision'],
                       info['metrics']['recall'],
                       info['metrics']['f1']],
                    text=[f"{v*100:.1f}%" for v in [info['metrics']['accuracy'],
                                                    info['metrics']['precision'],
                                                    info['metrics']['recall'],
                                                    info['metrics']['f1']]],
                    textposition='auto',
                ))
            
            fig.update_layout(
                title='Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬',
                barmode='group',
                yaxis_title='Ø§Ù„Ù‚ÙŠÙ…Ø©',
                yaxis_tickformat='.0%'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            best = st.session_state.model_pack['best_model']
            st.success(f"ğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù‡Ùˆ: **{best}**")
            
        else:
            st.info("ğŸ‘ˆ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ========== Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… ==========
    with tabs[6]:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown('<div class="card-title">âš–ï¸ Ù…Ø­Ø§ÙƒÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø°ÙƒÙŠ</div>', unsafe_allow_html=True)
        
        if st.session_state.model_trained and st.session_state.model_pack:
            model_pack = st.session_state.model_pack
            
            st.markdown("#### ğŸ”® Ø§Ø®ØªØ± Ù‚Ø¶ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            
            col1, col2 = st.columns(2)
            
            with col1:
                if 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±' in df.columns:
                    decision_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø±Ø§Ø±", df['Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±'].dropna().unique())
                else:
                    decision_type = "Ø¬Ù†Ø§Ø¦ÙŠ"
                
                if 'Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
                    case_disp = st.selectbox("Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù‚Ø¶ÙŠØ©", df['Ù†ØªÙŠØ¬Ø©_Ø§Ù„Ù‚Ø¶ÙŠØ©'].dropna().unique())
                else:
                    case_disp = "Ù‚Ø¨ÙˆÙ„"
                
                evidence = st.slider("Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø© (1-5)", 1, 5, 3)
            
            with col2:
                if 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©' in df.columns:
                    judge = st.selectbox("Ø§Ù„Ù‚Ø§Ø¶ÙŠ", df['Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©'].dropna().unique())
                else:
                    judge = "Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø£Ø­Ù…Ø¯"
                
                precedent = st.selectbox("ØªØºÙŠÙŠØ± Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", [0, 1], format_func=lambda x: "Ù†Ø¹Ù…" if x == 1 else "Ù„Ø§")
                split = st.selectbox("ØªØµÙˆÙŠØª Ù…Ù†Ù‚Ø³Ù…", [0, 1], format_func=lambda x: "Ù†Ø¹Ù…" if x == 1 else "Ù„Ø§")
            
            case_text = st.text_input("Ù†Øµ Ø§Ù„Ù‚Ø¶ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ù‚Ø¶ÙŠØ© Ø±Ù‚Ù… 12345")
            
            if st.button("ğŸ”® ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø¶ÙŠØ©", use_container_width=True):
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
                best_model_name = model_pack['best_model']
                best_model_info = model_pack['models'][best_model_name]
                best_model = best_model_info['model']
                
                st.info(f"âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name}")
                
                # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯
                input_data = {
                    'Ù‚ÙˆØ©_Ø§Ù„Ø£Ø¯Ù„Ø©': evidence,
                    'ØªÙ…_Ø§Ù„Ù‚Ø¨Ø¶': 1,  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
                    'ØªØºÙŠÙŠØ±_Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©': precedent,
                    'ØªØµÙˆÙŠØª_Ù…Ù†Ù‚Ø³Ù…': split,
                    'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø«Ù‚Ø©': 0.8  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
                }
                
                corruption_prob, reasons = calculate_corruption_probability(input_data)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown(f"""
                    <div class="metric-neon">
                        <div class="metric-neon-value">{best_model_name}</div>
                        <div class="metric-neon-label">Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class="metric-neon">
                        <div class="metric-neon-value">{corruption_prob*100:.1f}%</div>
                        <div class="metric-neon-label">Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ³Ø§Ø¯</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    risk_level = "Ù…Ù†Ø®ÙØ¶Ø©" if corruption_prob < 0.3 else "Ù…ØªÙˆØ³Ø·Ø©" if corruption_prob < 0.6 else "Ø¹Ø§Ù„ÙŠØ©"
                    risk_color = "success" if corruption_prob < 0.3 else "warning" if corruption_prob < 0.6 else "danger"
                    
                    st.markdown(f"""
                    <div class="metric-neon" style="background: {'#10b981' if risk_level=='Ù…Ù†Ø®ÙØ¶Ø©' else '#f59e0b' if risk_level=='Ù…ØªÙˆØ³Ø·Ø©' else '#ef4444'}">
                        <div class="metric-neon-value">{risk_level}</div>
                        <div class="metric-neon-label">Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                if reasons:
                    st.warning(f"âš ï¸ Ø£Ø³Ø¨Ø§Ø¨ Ù…Ø­ØªÙ…Ù„Ø©: {', '.join(reasons)}")
                
                # Ø¥ÙŠØ¬Ø§Ø¯ Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KNN
                if 'KNN' in model_pack['models'] and 'Ø§Ø³Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©' in df.columns:
                    st.markdown("#### ğŸ” Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©")
                    
                    # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø­Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©
                    similar_cases = df.sample(min(5, len(df)))[['Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©', 'Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', 'Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±']].to_dict('records')
                    
                    for i, case in enumerate(similar_cases):
                        st.markdown(f"{i+1}. Ù‚Ø¶ÙŠØ© {case.get('Ø±Ù‚Ù…_Ø§Ù„Ù‚Ø¶ÙŠØ©', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')} - {case.get('Ù†ÙˆØ¹_Ø§Ù„Ù‚Ø±Ø§Ø±', '')} - Ø§Ù„Ù‚Ø§Ø¶ÙŠ {case.get('Ø±Ø¦ÙŠØ³_Ø§Ù„Ù…Ø­ÙƒÙ…Ø©', '')}")
        else:
            st.info("ğŸ‘ˆ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # Ø§Ù„ÙÙˆØªØ±
    st.markdown("""
    <div class="footer-advanced">
        <h3>âš–ï¸ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© - Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©</h3>
        <p>Ø§Ù„Ø¥ØµØ¯Ø§Ø± 5.0 | Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù€ Decision Tree, KNN, Ùˆ Vectors</p>
        <p style="margin-top: 2rem; opacity: 0.7;">Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø© Â© 2026</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
